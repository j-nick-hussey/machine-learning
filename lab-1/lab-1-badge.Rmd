---
title: 'Machine Learning - Lab 1 Independent Assignment'
author: "J. Nick Hussey"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

![](img/ML_P_Hx.jpg){width="30%"}

The lab provides space to work with data and to reflect on how the concepts and techniques introduced in each lab might apply to your own research.

To earn a badge for each lab, you are required to respond to a set of prompts for two parts:

-   In Part I, you will extend our model by adding another variable.

-   In Part II, you will reflect on your understanding of key concepts and begin to think about potential next steps for your own study.

### Part I: Extending our model

In this part of the badge activity, please add another variable -- a variable for the number of days before the start of the module students registered. This variable will be a **third predictor**. By adding it, you'll be able to examine how much *more accurate* your model is (if at al, as this variable might not have great predictive power). Note that this variable is a number and so no pre-processing is necessary.

In doing so, please move all of your code needed to run the analysis over from your case study file here. This is essential for your analysis to be reproducible. You may wish to break your code into multiple chunks based on the overall purpose of the code in the chunk (e.g., loading packages and data, wrangling data, and each of the machine learning steps).

#### 1. PREPARE

```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
```

#### 2. MUTATE

```{r}
students <- read_csv("data/oulad-students.csv")
glimpse(students)
```

```{r}
students <- students %>%
    mutate(pass = ifelse(final_result == "Pass", 1, 0)) %>% # creates a new variable named "pass" and a dummy code of 1 if value of final_result equals "pass" and 0 if not
    mutate(pass = as.factor(pass)) # makes the variable a factor, helping later steps

students <- students %>% 
    mutate(disability = as.factor(disability))

#view(students)
```

#### 3. EXPLORE

```{r}
students %>% 
    count(id_student) # this many students

students %>% 
    count(code_module, code_presentation) # this many offerings
```

```{r}
students <- students %>% 
    mutate(imd_band = factor(imd_band, levels = c("0-10%",
                                                  "10-20%",
                                                  "20-30%",
                                                  "30-40%",
                                                  "40-50%",
                                                  "50-60%",
                                                  "60-70%",
                                                  "70-80%",
                                                  "80-90%",
                                                  "90-100%"))) %>% # this creates a factor with ordered levels
    mutate(imd_band = as.integer(imd_band)) # this changes the levels into integers based on the order of the factor levels

students
```

#### 4. MODEL

##### 4.1 Split Data

```{r}
set.seed(20230712)
train_test_split <- initial_split(students, prop = .80)
data_train <- training(train_test_split)
data_test  <- testing(train_test_split)

data_train
data_test
```

##### 4.2 Recipe

```{r}
my_rec <- recipe(pass ~ disability + imd_band + date_registered, data = data_train)

my_rec
```

##### 4.3 Specify

```{r}
my_mod <-
    logistic_reg()

my_mod <-
    logistic_reg() %>% 
    set_engine("glm") %>% # generalized linear model
    set_mode("classification") # since we are predicting a dichotomous outcome, specify classification; for a number, specify regression

my_wf <-
    workflow() %>% # create a workflow
    add_model(my_mod) %>% # add the model we wrote above
    add_recipe(my_rec) # add our recipe we wrote above
```

##### 4.4 Fit

```{r}
fitted_model <- fit(my_wf, data = data_train)
fitted_model

final_fit <- last_fit(my_mod, my_rec, split = train_test_split)
```

4.5 Interpret

```{r}
final_fit %>%
    collect_predictions()

final_fit %>% 
    collect_predictions() %>% # see test set predictions
    select(.pred_class, pass) %>% # just to make the output easier to view 
    mutate(correct = .pred_class == pass) # create a new variable, correct, telling us when the model was and was not correct


final_fit %>% 
    collect_predictions() %>% # see test set predictions
    select(.pred_class, pass) %>% # just to make the output easier to view 
    mutate(correct = .pred_class == pass) %>% # create a new variable, correct, telling us when the model was and was not correct
    tabyl(correct)
```

Previous results: TRUE 53.5051%

New results: TRUE 53.4284%

How does the accuracy of this new model compare? Add a few reflections below:

### Part II: Reflect and Plan

Part A: Please refer back to Breiman's (2001) article for these three questions.

1.  Can you summarize the primary difference between the two cultures of statistical modeling that Breiman outlines in his paper?

-   Leo Breiman defines the two cultures of statistical modeling as thedata modeling culture and the algorithmic culture in his paper *Statistical Modeling: The Two Cultures.* Both cultures see data as being created by a black box with an input of x (independent variables) and an output of y (response variables). The data modeling culture seeks to find the and understand the underlying black box of a selected model. In contrast, the algorithmic culture views the understanding of the black box as unnecessary and wishes to find an algorithm to make predictions and gain information.

2.  How has the advent of big data and machine learning affected or reinforced Breiman's argument since the article was published?

-   Breiman argues that the existence of these two cultures places unnecessary barriers that hamper the progress of data science. Statisticians should focus on the goals of prediction accuracy and knowledge instead of determining the most appropriate model. His argument is reinforced as big data and machine learning have, obviously, increased the speed and volume of all data. Thus, not losing focus of the goals in all of the noise is paramount. Simply put, there is a place for both cultures but context matters.

3.  Breiman emphasized the importance of predictive accuracy over understanding why a method works. To what extent do you agree or disagree with this stance?

-   I see a valid argument for both sides and instinctively want to state understanding as more important. However, when accounting for the 4 V's of big data (velocity, veracity, volume, and variety) and increasing complexity of machine learning, the problems and methods are growing beyond a point of understanding for all but the most talented data scientists. Hence, predictive accuracy outweighs understanding. This is stated at a moment in time approximately 20 years after Breiman made the observation. Another 20 years will likely push the capabilities of machine learning beyond human comprehension.

Part B:

1.  How good was the machine learning model you developed in the badge activity? What if you read about someone using such a model as a reviewer of research? Please add your thoughts and reflections following the bullet point below.

-   Both models use if a student claims a disability and imb_band (a poverty measurement), where the new model adds date_registration (number of days before the start of the class). Comparing the previous model to the new reveals a decrease of less than a tenth of a percent. Despite the decrease, the minute difference between the two results is negligible. Personally, I would appreciate to read a research paper on modeling such as this. However, if the research paper concluded with an accuracy result of approximately a coin flip, then I would be none too impressed. Additionally, the sensitive nature of disabilities and poverty on a measurement of a students academics would likely lead to a biased, offensive paper.

2.  How might the model be improved? Share any ideas you have at this time below:

-   After running the previous model with the remaining variables individually, the most accurate variables appear to be disability, imd_band, region. These predictors lead to a TRUE accuracy score of 53.4438%. This is, again, a minute difference to any previous models but represents the best third variable with the original model.

Part C: Use the institutional library (e.g. [NU Library](https://vlc.ent.sirsi.net/client/en_US/nor)), [Google Scholar](https://scholar.google.com/) or search engine to locate a research article, presentation, or resource that applies machine learning to an educational context aligned with your research interests. More specifically, **locate a machine learning study that involves making predictions**.

1.  Provide an APA citation for your selected study.

    -   Khor, E. T. (2019). PREDICTIVE MODELS WITH MACHINE LEARNING ALGORITHMS TO FORECAST STUDENTS’ PERFORMANCE. *INTED2019 Proceedings*, 2831–2837. <https://doi.org/10.21125/inted.2019.0757>

2.  What research questions were the authors of this study trying to address and why did they consider these questions important?

    -   The researcher had three goals with this case study on educational performance. (1) To accurately predict the academic category a student would fall in between high-achiever, middle-achiever, and low-achiever with the use of machine learning. Can performance be predicted? (2) To test if the inclusion of learning behavioral variables, like number of times student raised questions or number of absences, affect the accuracy. Does behavior affect performance? (3) To compare the accuracy with the same variables across 4 models (decision tree, the naïve Bayes, the support vector machine, and the neural network). Which model(s) produce accurate results?

        The author clearly identifies the potential outcome of early identification of achievement potential by stating “at-risk students can be identified as early as the beginning of the course and they can be provided in-time intervention” (Khor, 2019).

3.  What were the results of these analyses?

    -   The paper produces fascinating results when it compares the performance of the 4 models. All 4 models revealed significantly higher results when incorporating learning behavior variables compared to not with an average increase of 18.28%. The support vector machine model produces the highest accuracy with 78.75% edging out the decision tree (78.33%) and neural network (75.83%) models. The Naïve Bayes underperformed at 67.71%, while the other 3 could reliably predict the achievement potential of 3 out of every 4 students.

### Knit and Publish

Complete the following steps to knit and publish your work:

1.  First, change the name of the `author:` in the [YAML header](https://bookdown.org/yihui/rmarkdown-cookbook/rmarkdown-anatomy.html#yaml-metadata) at the very top of this document to your name. The YAML header controls the style and feel for knitted document but doesn't actually display in the final output.

2.  Next, click the knit button in the toolbar above to "knit" your R Markdown document to a [HTML](https://bookdown.org/yihui/rmarkdown/html-document.html) file that will be saved in your R Project folder. You should see a formatted webpage appear in your Viewer tab in the lower right pan or in a new browser window. Let your instructor know if you run into any issues with knitting.

3.  Finally, publish your webpage on Rpubs by clicking the "Publish" button located in the Viewer Pane after you knit your document.

### Your First Machine Learning Badge

Congratulations, you've completed your first badge activity! To receive credit for this assignment and earn your first official Lab Badge, submit the link on Blackboard and share with your instructor.

Once your instructor has checked your link, you will be provided a physical version of the badge below!

![](img/ML_P_Hx.jpg){width="30%"}
